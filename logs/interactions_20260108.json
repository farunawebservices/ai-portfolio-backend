[
  {
    "timestamp": "2026-01-10T14:01:50.054423",
    "session_id": "e3159ae8-0a85-4331-a938-71a9b26ac91d",
    "mode": "default",
    "question": "how are getting database?",
    "answer": "That's a great question about how I gathered the data for my projects, especially the Igala dataset. For the Igala Low-Resource Language Dataset Explorer, I actually went through a process of **field collection** to gather those 3,253 sentences since there weren't many existing digital resources available. After collection, a significant part of the work involved rigorous **dataset auditing and linguistic statistical analysis** to ensure the quality and usefulness of the data for future NLP tasks. It was definitely a challenge working with limited initial resources, but that process was key to creating what I hope will be a foundational resource.",
    "error": null,
    "response_time_seconds": 5.48,
    "model": "models/gemini-flash-lite-latest"
  },
  {
    "timestamp": "2026-01-10T14:02:40.578554",
    "session_id": "e3159ae8-0a85-4331-a938-71a9b26ac91d",
    "mode": "default",
    "question": "tell me about Faruna Godwin",
    "answer": "That's a lovely question! Essentially, I'm Faruna Godwin Abuh, an AI Research Engineer driven by a real passion for building AI that is both safe and beneficial for everyone, especially within African communities. My core focus areas really revolve around making AI systems more transparent\u2014which is where my work in interpretability comes in\u2014and ensuring that languages like Igala aren't left behind in the NLP revolution. I believe that responsible AI development means actively contributing to both the safety and linguistic diversity of the field.",
    "error": null,
    "response_time_seconds": 1.62,
    "model": "models/gemini-flash-lite-latest"
  },
  {
    "timestamp": "2026-01-10T14:03:53.549684",
    "session_id": "e3159ae8-0a85-4331-a938-71a9b26ac91d",
    "mode": "deep-dive",
    "question": "how did you build this app?",
    "answer": "That's an excellent follow-up question! When you ask how I built the **Igala Low-Resource Language Dataset Explorer**, you are asking about the technical pipeline used to transform raw, field-collected linguistic data into an interactive, auditable application.\n\nHere is a detailed, technical breakdown of the architecture and implementation:\n\n### 1. Data Ingestion and Processing Pipeline\n\nThe foundation of the Explorer is the data itself, which required robust processing before visualization.\n\n**Technologies Used:** Python, Pandas, foundational NLP libraries (e.g., NLTK for initial tokenization sanity checks).\n\n**Methodology:**\n1.  **Data Structuring:** The 3,253 Igala sentences, initially collected via manual transcription and digitization, were ingested into a structured format (CSV/JSON lines).\n2.  **Pandas for Auditing:** I leveraged **Pandas DataFrames** for all data auditing and preliminary analysis. This was critical for quality control.\n    *   **Token Analysis:** I implemented custom functions vectorized across the DataFrame to calculate basic linguistic statistics. For instance, calculating the **Type-Token Ratio (TTR)** across the corpus involved iterating through tokenized columns and applying aggregation functions. This helped quantify vocabulary richness.\n    *   **Length Distribution Analysis:** Calculating sentence length histograms (in tokens and characters) involved vectorized operations on string lengths, which is significantly faster than pure Python loops.\n3.  **Linguistic Statistics:** Beyond simple counts, I performed basic morphology checks (where possible without a pre-existing morphological analyzer for Igala) by analyzing common affix patterns identified during the field collection phase, tagging sentences based on identified grammatical features.\n\n### 2. Application Architecture and Framework Selection\n\nThe goal was rapid prototyping and deployment tailored for the NLP community, prioritizing ease of use over complex backend infrastructure.\n\n**Architecture Decision:** A lightweight, Python-native web application framework was chosen.\n\n**Technology Chosen:** **Streamlit**.\n\n**Trade-offs:**\n*   **Pros:** Extremely fast development cycle; seamless integration with Pandas DataFrames (data visualizations rendered directly from analysis results); native support for simple, component-based UI.\n*   **Cons:** Less control over fine-grained state management and complex routing compared to a full-stack framework like Django or FastAPI/React. For an internal data explorer, this trade-off was acceptable.\n\n### 3. Implementation Details (Code-Level Insights)\n\nThe Streamlit application structure followed a modular approach:\n\n**A. Data Loading and Caching:**\nTo ensure performance, especially when running statistical queries, I implemented Streamlit\u2019s caching mechanism:\n\n```python\n@st.cache_data\ndef load_igala_data(filepath=\"igala_corpus.csv\"):\n    # Load data using Pandas\n    df = pd.read_csv(filepath)\n    \n    # Pre-calculate static metrics once\n    df['token_count'] = df['sentence'].apply(lambda x: len(x.split()))\n    \n    # Perform initial tokenization (simple whitespace splitting for exploration)\n    df['tokens'] = df['sentence'].apply(lambda x: x.split())\n    \n    return df\n\nigala_df = load_igala_data()\n```\n\n**B. Interactive Exploration Components:**\nI mapped key analytical outputs directly to Streamlit widgets:\n\n1.  **Sidebar Filters:** Used `st.sidebar.multiselect` to allow users to filter the DataFrame based on metadata tags (e.g., source domain, validated speaker). This dynamically re-renders the displayed DataFrame and statistics based on the subset.\n2.  **Token Visualization:** I used Streamlit\u2019s charting capabilities, often leveraging underlying libraries like **Altair** (which Streamlit wraps effectively) to display frequency distributions of the most common trigrams and bigrams identified from the tokenized data. For example, visualizing the top 50 tokens required passing the Series output of `igala_df['tokens'].value_counts().head(50)` directly to `st.bar_chart()`.\n\n**C. Deployment Strategy:**\nThe final artifact was deployed directly to **Hugging Face Spaces**. This leveraged HF's integration capabilities, allowing the Streamlit application to run within their ephemeral container environment, making the dataset immediately accessible to the broader NLP research community without requiring users to set up local Python environments or deal with cloud infrastructure.\n\nIn summary, the app is an **interactive Pandas front-end built with Streamlit**, designed specifically to democratize access to the audit-validated Igala linguistic data, fulfilling my commitment to both low-resource NLP and community accessibility.",
    "error": null,
    "response_time_seconds": 4.37,
    "model": "models/gemini-flash-lite-latest"
  }
]